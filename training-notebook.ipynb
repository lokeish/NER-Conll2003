{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install transformers datasets tokenizers sequel -q\n! pip install -U accelerate\n! pip install -U transformers\n! pip install seqeval","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-04T15:00:37.885272Z","iopub.execute_input":"2023-07-04T15:00:37.885631Z","iopub.status.idle":"2023-07-04T15:01:36.854701Z","shell.execute_reply.started":"2023-07-04T15:00:37.885603Z","shell.execute_reply":"2023-07-04T15:01:36.853558Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.12.0)\nCollecting accelerate\n  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.4.1)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.0.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (3.12.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\nInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.12.0\n    Uninstalling accelerate-0.12.0:\n      Successfully uninstalled accelerate-0.12.0\nSuccessfully installed accelerate-0.20.3\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.30.1)\nCollecting transformers\n  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (5.4.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.5.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.28.2)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\nInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.30.1\n    Uninstalling transformers-4.30.1:\n      Successfully uninstalled transformers-4.30.1\nSuccessfully installed transformers-4.30.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting seqeval\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.23.5)\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.2.2)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.10.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\nBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=63ac34597268ce48ca05c0484d953baf990af34501dfb3ce3900d47784b0d2fb\n  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\nSuccessfully built seqeval\nInstalling collected packages: seqeval\nSuccessfully installed seqeval-1.2.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# Required Imports\nimport datasets\nimport numpy as np\nfrom transformers import BertTokenizerFast, ElectraTokenizerFast\nfrom transformers import DataCollatorForTokenClassification\nfrom transformers import AutoModelForTokenClassification\nfrom transformers import TrainingArguments, Trainer\n\n\nclass NERTraining:\n    def __init__(self, model_name) -> None:\n        self.model_name = model_name\n\n\n    def get_dataset(self, dataset_name: str):\n        \"\"\"Downloads dataset from hugging face\"\"\" \n        dataset = None\n        try: \n            dataset = datasets.load_dataset(dataset_name)\n        except Exception as ex:\n            print(\"Unable to download dataset - \", ex)\n\n        return dataset\n\n    def get_tokenizer(self):\n        \"\"\"Returns the tokenizer based on the model selected\"\"\"\n        tokenizer = None\n        try:\n            if self.model_name.lower() == \"bert-base-uncased\":\n                tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n            elif \"electra\" in self.model_name:\n                tokenizer = ElectraTokenizerFast.from_pretrained(\"bert-base-uncased\")\n        except Exception as ex:\n            print(\"Unable to get tokenizer for the model -%s\", self.model_name)\n\n        return tokenizer\n\n\n    def format_labels(self, data,  label_all=True):\n        \"\"\"\n        Appends -100 for the None Type and returns the labels\n        \"\"\"\n        tokenizer = self.get_tokenizer()\n        tokenized_input = tokenizer(data['tokens'], truncation=True, is_split_into_words=True)\n        labels = []\n        for i, label in enumerate(data['ner_tags']):\n            word_ids = tokenized_input.word_ids(batch_index=i)\n            label_ids = []\n            pre_ind = None\n\n            for wi in word_ids:\n                if wi is None:\n                    label_ids.append(-100)\n                elif wi != pre_ind:\n                    label_ids.append(label[wi])\n                else:\n                    label_ids.append(label[wi] if label_all else -100)\n\n                pre_ind = wi\n\n            # now append to labels list\n            labels.append(label_ids)\n\n        tokenized_input['labels'] = labels\n\n        return tokenized_input\n\n\n    def get_model(self):\n        \"\"\"Returns the model instance\"\"\"\n        model = None\n        try:\n            model = AutoModelForTokenClassification.from_pretrained(self.model_name, num_labels=9)\n        except Exception as ex:\n            print(\"Unable to download the model - \",self.model_name)\n          \n        return model\n\n    def set_arguments(self, m_args:dict):\n        \"\"\"Based on give settings create args object\"\"\"\n        args = None\n        try:\n            args = TrainingArguments(**m_args)\n        except Exception as ex:\n            print(\"Unable to create args object based on the provided - \", ex)\n        return args \n\n    def get_data_collator(self, tokenizer):\n        \"\"\"data collator \"\"\"\n        data_collator = None\n        try:\n            data_collator = DataCollatorForTokenClassification(tokenizer)\n        except Exception as ex:\n            print(\"Data collator operation failed - \", ex)\n\n        return data_collator\n\n    def get_metrics(self):\n        metrics = None\n        try:\n           metrics = datasets.load_metric(\"seqeval\")\n        except Exception as ex:\n            print(\"Unable to load metrics from seqeval - \", ex)\n        return metrics\n\n    def compute_metrics(self, p):\n        \"\"\"computest result for the prediction and actual output\"\"\"\n        label_list = dataset['train'].features['ner_tags'].feature.names\n        metrics = self.get_metrics()\n        predictions, labels = p\n        #select predicted index with maximum logit for each token\n        predictions = np.argmax(predictions, axis=2)\n\n        # model predictions\n        true_predictions = [\n            [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n            for prediction, label in zip(predictions, labels)\n        ]\n\n        # actual prediction\n        true_labels = [\n            [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n            for prediction, label in zip(predictions, labels)\n        ]\n\n        # compute result\n        results = metrics.compute(predictions=true_predictions, references=true_labels)\n\n        result_dict =  {\n            \"precision\": results[\"overall_precision\"],\n            \"recall\": results[\"overall_recall\"],\n            \"f1\": results[\"overall_f1\"],\n            \"accuracy\": results[\"overall_accuracy\"],\n        }\n\n        return result_dict\n\n    def model_training(self, model, args, train_dataset, eval_dataset, data_collator, tokenizer, compute_metrics):\n        \"\"\"Trains the model based on give params\"\"\"\n        try:\n            trainer = Trainer(\n                                model,\n                                args,\n                                train_dataset=train_dataset,\n                                eval_dataset=eval_dataset,\n                                data_collator=data_collator,\n                                tokenizer=tokenizer,\n                                compute_metrics=compute_metrics\n                            )\n            trainer.train()\n\n        except Exception as ex:\n            print(\"Unable to train the model - \", ex)\n\n        return trainer\n\n    def save_artifacts(model, tokenizer, model_name, tokenizer_name):\n        \"\"\"Save artifacts for the model predictions\"\"\"\n        model.save_pretrained(model_name)\n        tokenizer.save_pretrained(tokenizer_name)\n\n    def save_model(self, model, tokenizer, loc_name, label_list):\n        \"\"\"saves the artificats to given location\"\"\"\n        model.save_pretrained(loc_name)\n        tokenizer.save_pretrained(\"tokenizer\")\n        print(\"Successfully saved the model :)\")\n\n","metadata":{"execution":{"iopub.status.busy":"2023-07-04T15:01:59.746777Z","iopub.execute_input":"2023-07-04T15:01:59.747139Z","iopub.status.idle":"2023-07-04T15:02:14.468436Z","shell.execute_reply.started":"2023-07-04T15:01:59.747103Z","shell.execute_reply":"2023-07-04T15:02:14.467546Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"train_obj =  NERTraining(\"bert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2023-07-04T15:02:46.008439Z","iopub.execute_input":"2023-07-04T15:02:46.009121Z","iopub.status.idle":"2023-07-04T15:02:46.013742Z","shell.execute_reply.started":"2023-07-04T15:02:46.009084Z","shell.execute_reply":"2023-07-04T15:02:46.012720Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Dataset download\ndataset = train_obj.get_dataset(\"conll2003\")\n# tokenizer \ntokenizer = train_obj.get_tokenizer()\n# tokenized dataset\ntokenized_dataset = dataset.map(train_obj.format_labels, batched=True)\n# Training args\nm_args =  {\"output_dir\": \"ner-final-model\",\n         \"evaluation_strategy\": \"epoch\",\n         \"learning_rate\": 2e-05,\n         \"per_device_eval_batch_size\": 16,\n         \"per_device_train_batch_size\": 16,\n         \"num_train_epochs\": 3,\n         \"weight_decay\": 0.01\n         }\ntrain_args = train_obj.set_arguments(m_args)\n\n# data collator\ndata_cltr = train_obj.get_data_collator(tokenizer)\n\n# metrics\nmetrics = train_obj.get_metrics()\n\nlabel_list = dataset['train'].features['ner_tags'].feature.names\n\n# model \nmodel = train_obj.get_model()\n\n# model training\ntrain = train_obj.model_training(\n    model,\n    train_args,\n    train_dataset=tokenized_dataset['train'],\n    eval_dataset=tokenized_dataset['validation'],\n    data_collator=data_cltr,\n    tokenizer=tokenizer,\n    compute_metrics=train_obj.compute_metrics)\n# save the model\ntrain_obj.save_model(model, tokenizer, \"artifacts\", {})","metadata":{"execution":{"iopub.status.busy":"2023-07-04T15:02:48.600126Z","iopub.execute_input":"2023-07-04T15:02:48.600513Z","iopub.status.idle":"2023-07-04T15:14:55.186424Z","shell.execute_reply.started":"2023-07-04T15:02:48.600466Z","shell.execute_reply":"2023-07-04T15:14:55.185559Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9808666371624667a525e5d898e0e1b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/1.62k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd21d07bd8a34e9d848e0223f6840ee5"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset conll2003/conll2003 (download: 959.94 KiB, generated: 9.78 MiB, post-processed: Unknown size, total: 10.72 MiB) to /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/983k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8403df71605a43879e7cc2999fb72d40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/14042 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3251 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/3454 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset conll2003 downloaded and prepared to /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bafb362422d4dd9a9dd2787ef6df469"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f08d148b03e4fc28ae8dbc9cff8ba39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4817567d03854c1a87b46e743fedaf05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"832d63e3efdd480db06e99174c1392f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83fa281253c34421aac5eef6fd4bfc76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/15 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1d9156e4e314e51b07f61ccebf83e0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac4ef26de03040d6a02b76fa5b0fe0bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6137969e33a34cafa7a83324c552d33e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.47k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"032cee95d4f240dc9a8b99d97209580a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"866d437813144f1da5d30669c0a49b12"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230704_150747-1g8tx2gb</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/lokeshdesai7/huggingface/runs/1g8tx2gb' target=\"_blank\">vital-resonance-4</a></strong> to <a href='https://wandb.ai/lokeshdesai7/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/lokeshdesai7/huggingface' target=\"_blank\">https://wandb.ai/lokeshdesai7/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/lokeshdesai7/huggingface/runs/1g8tx2gb' target=\"_blank\">https://wandb.ai/lokeshdesai7/huggingface/runs/1g8tx2gb</a>"},"metadata":{}},{"name":"stderr","text":"You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1317' max='1317' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1317/1317 06:28, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.070567</td>\n      <td>0.904509</td>\n      <td>0.915539</td>\n      <td>0.909991</td>\n      <td>0.980396</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.189500</td>\n      <td>0.062722</td>\n      <td>0.919594</td>\n      <td>0.931424</td>\n      <td>0.925471</td>\n      <td>0.983017</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.050000</td>\n      <td>0.059927</td>\n      <td>0.925647</td>\n      <td>0.935899</td>\n      <td>0.930745</td>\n      <td>0.984272</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"name":"stdout","text":"Successfully saved the model :)\n","output_type":"stream"}]},{"cell_type":"code","source":"label_list = dataset[\"train\"].features[\"ner_tags\"].feature.names ","metadata":{"execution":{"iopub.status.busy":"2023-07-04T15:16:42.290365Z","iopub.execute_input":"2023-07-04T15:16:42.291399Z","iopub.status.idle":"2023-07-04T15:16:42.300641Z","shell.execute_reply.started":"2023-07-04T15:16:42.291355Z","shell.execute_reply":"2023-07-04T15:16:42.299744Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"id2label = {\n    str(i): label for i,label in enumerate(label_list)\n}\nlabel2id = {\n    label: str(i) for i,label in enumerate(label_list)\n}","metadata":{"execution":{"iopub.status.busy":"2023-07-04T15:17:29.214633Z","iopub.execute_input":"2023-07-04T15:17:29.214997Z","iopub.status.idle":"2023-07-04T15:17:29.221166Z","shell.execute_reply.started":"2023-07-04T15:17:29.214969Z","shell.execute_reply":"2023-07-04T15:17:29.220071Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import json\n\nconfig = json.load(open(\"artifacts/config.json\"))\nconfig[\"id2label\"] = id2label\nconfig[\"label2id\"] = label2id\njson.dump(config, open(\"artifacts/config.json\",\"w\"))\n     \n","metadata":{"execution":{"iopub.status.busy":"2023-07-04T15:17:30.254220Z","iopub.execute_input":"2023-07-04T15:17:30.254800Z","iopub.status.idle":"2023-07-04T15:17:30.276251Z","shell.execute_reply.started":"2023-07-04T15:17:30.254760Z","shell.execute_reply":"2023-07-04T15:17:30.267683Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}