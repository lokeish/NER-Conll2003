{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-07-03T11:32:40.144684Z","iopub.status.busy":"2023-07-03T11:32:40.144292Z","iopub.status.idle":"2023-07-03T11:33:39.506923Z","shell.execute_reply":"2023-07-03T11:33:39.505516Z","shell.execute_reply.started":"2023-07-03T11:32:40.144651Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.12.0)\n","Collecting accelerate\n","  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.4.1)\n","Requirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.0.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.0.9)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (3.12.0)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (4.5.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (3.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->accelerate) (3.1.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n","Installing collected packages: accelerate\n","  Attempting uninstall: accelerate\n","    Found existing installation: accelerate 0.12.0\n","    Uninstalling accelerate-0.12.0:\n","      Successfully uninstalled accelerate-0.12.0\n","Successfully installed accelerate-0.20.3\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.30.1)\n","Collecting transformers\n","  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.1)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (5.4.1)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.5.5)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.28.2)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.1)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.64.1)\n","Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\n","Installing collected packages: transformers\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.30.1\n","    Uninstalling transformers-4.30.1:\n","      Successfully uninstalled transformers-4.30.1\n","Successfully installed transformers-4.30.2\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mCollecting seqeval\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.23.5)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.2.2)\n","Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.10.1)\n","Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n","Building wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=6702ed234b7a5f93c5d847e9b4549cffa99209a9a2bc3b6eafa5dc407094cddc\n","  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n","Successfully built seqeval\n","Installing collected packages: seqeval\n","Successfully installed seqeval-1.2.2\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["! pip install transformers datasets tokenizers sequel -q\n","! pip install -U accelerate\n","! pip install -U transformers\n","! pip install seqeval"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T11:46:11.128356Z","iopub.status.busy":"2023-07-03T11:46:11.127446Z","iopub.status.idle":"2023-07-03T11:46:11.160359Z","shell.execute_reply":"2023-07-03T11:46:11.158738Z","shell.execute_reply.started":"2023-07-03T11:46:11.128320Z"},"trusted":true},"outputs":[],"source":["# Required Imports\n","import datasets\n","import numpy as np\n","from transformers import BertTokenizerFast, ElectraTokenizerFast\n","from transformers import DataCollatorForTokenClassification\n","from transformers import AutoModelForTokenClassification\n","from transformers import TrainingArguments, Trainer\n","\n","\n","class NERTraining:\n","    def __init__(self, model_name) -> None:\n","        self.model_name = model_name\n","\n","\n","    def get_dataset(self, dataset_name: str):\n","        \"\"\"Downloads dataset from hugging face\"\"\" \n","        dataset = None\n","        try: \n","            dataset = datasets.load_dataset(dataset_name)\n","        except Exception as ex:\n","            print(\"Unable to download dataset - \", ex)\n","\n","        return dataset\n","\n","    def get_tokenizer(self):\n","        \"\"\"Returns the tokenizer based on the model selected\"\"\"\n","        tokenizer = None\n","        try:\n","            if self.model_name.lower() == \"bert-base-uncased\":\n","                tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n","            elif \"electra\" in self.model_name:\n","                tokenizer = ElectraTokenizerFast.from_pretrained(\"bert-base-uncased\")\n","        except Exception as ex:\n","            print(\"Unable to get tokenizer for the model -%s\", self.model_name)\n","\n","        return tokenizer\n","\n","\n","    def format_labels(self, data,  label_all=True):\n","        \"\"\"\n","        Appends -100 for the None Type and returns the labels\n","        \"\"\"\n","        tokenizer = self.get_tokenizer()\n","        tokenized_input = tokenizer(data['tokens'], truncation=True, is_split_into_words=True)\n","        labels = []\n","        for i, label in enumerate(data['ner_tags']):\n","            word_ids = tokenized_input.word_ids(batch_index=i)\n","            label_ids = []\n","            pre_ind = None\n","\n","            for wi in word_ids:\n","                if wi is None:\n","                    label_ids.append(-100)\n","                elif wi != pre_ind:\n","                    label_ids.append(label[wi])\n","                else:\n","                    label_ids.append(label[wi] if label_all else -100)\n","\n","                pre_ind = wi\n","\n","            # now append to labels list\n","            labels.append(label_ids)\n","\n","        tokenized_input['labels'] = labels\n","\n","        return tokenized_input\n","\n","\n","    def get_model(self):\n","        \"\"\"Returns the model instance\"\"\"\n","        model = None\n","        try:\n","            model = AutoModelForTokenClassification.from_pretrained(self.model_name, num_labels=9)\n","        except Exception as ex:\n","            print(\"Unable to download the model - \",self.model_name)\n","          \n","        return model\n","\n","    def set_arguments(self, m_args:dict):\n","        \"\"\"Based on give settings create args object\"\"\"\n","        args = None\n","        try:\n","            args = TrainingArguments(**m_args)\n","        except Exception as ex:\n","            print(\"Unable to create args object based on the provided - \", ex)\n","        return args \n","\n","    def get_data_collator(self, tokenizer):\n","        \"\"\"data collator \"\"\"\n","        data_collator = None\n","        try:\n","            data_collator = DataCollatorForTokenClassification(tokenizer)\n","        except Exception as ex:\n","            print(\"Data collator operation failed - \", ex)\n","\n","        return data_collator\n","\n","    def get_metrics(self):\n","        metrics = None\n","        try:\n","           metrics = datasets.load_metric(\"seqeval\")\n","        except Exception as ex:\n","            print(\"Unable to load metrics from seqeval - \", ex)\n","        return metrics\n","\n","    def compute_metrics(self, p):\n","        \"\"\"computest result for the prediction and actual output\"\"\"\n","        label_list = dataset['train'].features['ner_tags'].feature.names\n","        metrics = self.get_metrics()\n","        predictions, labels = p\n","        #select predicted index with maximum logit for each token\n","        predictions = np.argmax(predictions, axis=2)\n","\n","        # model predictions\n","        true_predictions = [\n","            [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n","            for prediction, label in zip(predictions, labels)\n","        ]\n","\n","        # actual prediction\n","        true_labels = [\n","            [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n","            for prediction, label in zip(predictions, labels)\n","        ]\n","\n","        # compute result\n","        results = metrics.compute(predictions=true_predictions, references=true_labels)\n","\n","        result_dict =  {\n","            \"precision\": results[\"overall_precision\"],\n","            \"recall\": results[\"overall_recall\"],\n","            \"f1\": results[\"overall_f1\"],\n","            \"accuracy\": results[\"overall_accuracy\"],\n","        }\n","\n","        return result_dict\n","\n","    def model_training(self, model, args, train_dataset, eval_dataset, data_collator, tokenizer, compute_metrics):\n","        \"\"\"Trains the model based on give params\"\"\"\n","        try:\n","            trainer = Trainer(\n","                                model,\n","                                args,\n","                                train_dataset=train_dataset,\n","                                eval_dataset=eval_dataset,\n","                                data_collator=data_collator,\n","                                tokenizer=tokenizer,\n","                                compute_metrics=compute_metrics\n","                            )\n","            trainer.train()\n","\n","        except Exception as ex:\n","            print(\"Unable to train the model - \", ex)\n","\n","        return trainer\n","\n","    def save_artifacts(model, tokenizer, model_name, tokenizer_name):\n","        \"\"\"Save artifacts for the model predictions\"\"\"\n","        model.save_pretrained(model_name)\n","        tokenizer.save_pretrained(tokenizer_name)\n","\n","    def save_model(self, model, tokenizer, loc_name, label_list):\n","        \"\"\"saves the artificats to given location\"\"\"\n","        model.save_pretrained(loc_name)\n","        tokenizer.save_pretrained(\"tokenizer\")\n","        print(\"Successfully saved the model :)\")\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T11:46:27.932153Z","iopub.status.busy":"2023-07-03T11:46:27.931389Z","iopub.status.idle":"2023-07-03T11:46:27.938908Z","shell.execute_reply":"2023-07-03T11:46:27.937636Z","shell.execute_reply.started":"2023-07-03T11:46:27.932119Z"},"trusted":true},"outputs":[],"source":["train_obj =  NERTraining(\"bert-base-uncased\")"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T09:59:29.346490Z","iopub.status.busy":"2023-07-03T09:59:29.346039Z","iopub.status.idle":"2023-07-03T09:59:29.356150Z","shell.execute_reply":"2023-07-03T09:59:29.354975Z","shell.execute_reply.started":"2023-07-03T09:59:29.346449Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'google/electra-small-discriminator'"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":[]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T11:34:32.855112Z","iopub.status.busy":"2023-07-03T11:34:32.854745Z","iopub.status.idle":"2023-07-03T11:42:25.679086Z","shell.execute_reply":"2023-07-03T11:42:25.678138Z","shell.execute_reply.started":"2023-07-03T11:34:32.855082Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"18ca35c1ed384097bf7661299b625384","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/2.58k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"144106288e884a7586077806dcb0a350","version_major":2,"version_minor":0},"text/plain":["Downloading metadata:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Downloading and preparing dataset conll2003/conll2003 (download: 959.94 KiB, generated: 9.78 MiB, post-processed: Unknown size, total: 10.72 MiB) to /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8c3c858eafd64ea88e388c0e2e294207","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/983k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/14042 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Generating validation split:   0%|          | 0/3251 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Generating test split:   0%|          | 0/3454 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Dataset conll2003 downloaded and prepared to /root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee. Subsequent calls will reuse this data.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"929be25d0b9a436ca7d23c02f5d8d1bd","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"07d15fe89346414cac1272552cb5fca2","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c60d35d0b58e4cb28b16fed93db9c2c1","version_major":2,"version_minor":0},"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bba6550ed3de445099a80df7d24f8feb","version_major":2,"version_minor":0},"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"aa567929200949e68ab27d6238b899bb","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b16cb21e35a549c4a41425fd42843cbc","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/15 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ec604cb63b634878a98d6602009a41bb","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/4 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"02d2a5fffd9a4c07a92002f860b8d843","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/4 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3bc359c0a028442daa445624079b18ae","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/2.47k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"48fa419e9bd14c8baeae71283127a142","version_major":2,"version_minor":0},"text/plain":["Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  ········································\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/html":["Tracking run with wandb version 0.15.4"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20230703_113516-st13egiq</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/lokeshdesai7/huggingface/runs/st13egiq' target=\"_blank\">balmy-paper-2</a></strong> to <a href='https://wandb.ai/lokeshdesai7/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/lokeshdesai7/huggingface' target=\"_blank\">https://wandb.ai/lokeshdesai7/huggingface</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/lokeshdesai7/huggingface/runs/st13egiq' target=\"_blank\">https://wandb.ai/lokeshdesai7/huggingface/runs/st13egiq</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1317' max='1317' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1317/1317 06:31, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>0.069241</td>\n","      <td>0.904909</td>\n","      <td>0.919790</td>\n","      <td>0.912288</td>\n","      <td>0.980317</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.187600</td>\n","      <td>0.063030</td>\n","      <td>0.920150</td>\n","      <td>0.933326</td>\n","      <td>0.926691</td>\n","      <td>0.982859</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.050000</td>\n","      <td>0.059299</td>\n","      <td>0.927416</td>\n","      <td>0.936234</td>\n","      <td>0.931804</td>\n","      <td>0.984209</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]}],"source":["# Dataset download\n","dataset = train_obj.get_dataset(\"conll2003\")\n","# tokenizer \n","tokenizer = train_obj.get_tokenizer()\n","# tokenized dataset\n","tokenized_dataset = dataset.map(train_obj.format_labels, batched=True)\n","# Training args\n","m_args =  {\"output_dir\": \"ner-final-model\",\n","         \"evaluation_strategy\": \"epoch\",\n","         \"learning_rate\": 2e-05,\n","         \"per_device_eval_batch_size\": 16,\n","         \"per_device_train_batch_size\": 16,\n","         \"num_train_epochs\": 3,\n","         \"weight_decay\": 0.01\n","         }\n","train_args = train_obj.set_arguments(m_args)\n","\n","# data collator\n","data_cltr = train_obj.get_data_collator(tokenizer)\n","\n","# metrics\n","metrics = train_obj.get_metrics()\n","\n","label_list = dataset['train'].features['ner_tags'].feature.names\n","\n","# model \n","model = train_obj.get_model()\n","\n","# model training\n","train = train_obj.model_training(\n","    model,\n","    train_args,\n","    train_dataset=tokenized_dataset['train'],\n","    eval_dataset=tokenized_dataset['validation'],\n","    data_collator=data_cltr,\n","    tokenizer=tokenizer,\n","    compute_metrics=train_obj.compute_metrics)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-07-03T11:48:43.774031Z","iopub.status.busy":"2023-07-03T11:48:43.773653Z","iopub.status.idle":"2023-07-03T11:48:44.807470Z","shell.execute_reply":"2023-07-03T11:48:44.806594Z","shell.execute_reply.started":"2023-07-03T11:48:43.773994Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Successfully saved the model :)\n"]}],"source":["# save the model\n","train_obj.save_model(model, tokenizer, \"ner_final_model\", {})"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.5 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"},"vscode":{"interpreter":{"hash":"a6f1d5d85b27d2a4ffbc39dfdf4db9055b19e03093b052e6df23e32acb614efd"}}},"nbformat":4,"nbformat_minor":4}
